{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DatasetV2' has no attribute 'list_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[412], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mread_file(\u001b[39m'\u001b[39m\u001b[39mdatasets/train.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m test \u001b[39m=\u001b[39m  tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mlist_file(\u001b[39m'\u001b[39m\u001b[39mdatasets/train.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39m#data = pd.concat([train, test])\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'DatasetV2' has no attribute 'list_file'"
     ]
    }
   ],
   "source": [
    "train = tf.io.read_file('datasets/train.csv')\n",
    "test =  tf.data.Dataset.list_file('datasets/train.csv')\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = sns.FacetGrid(train)\n",
    "# grid.map_dataframe(sns.histplot)\n",
    "# plt.savefig('histograms.jpg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(df=train):\n",
    "    missing = df.isna().sum()\n",
    "    missing_data = missing[missing > 0]\n",
    "    return missing_data\n",
    "\n",
    "check_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Missing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YearBuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train[[\"GarageYrBlt\", \"YearBuilt\"]].corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"GarageYrBlt\"].fillna(train[\"YearBuilt\"],inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LotFrontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=train, x='LotFrontage',kind='strip', row='LotConfig', sharex=False,height=3,orient='portrait')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lot_front_dic = train.mask(train.LotFrontage.isna()) \\\n",
    "            .groupby('LotConfig') \\\n",
    "                ['LotFrontage'].median().round() \\\n",
    "                .to_dict()\n",
    "\n",
    "lot_front_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_LF = train[train.LotFrontage.isna()].index  \n",
    "\n",
    "for key, item in lot_front_dic.items():\n",
    "    train.loc[train['LotConfig']== key, 'LotFrontage'].fillna(item, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop 8 rows of ManVnrArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(axis = 'rows', subset=[\"MasVnrArea\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop categorical columns  with more than 600 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = int(train.shape[0]*.90)\n",
    "train.dropna(axis='columns', thresh= t, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns Dominated by a certain value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbalanced_cols = []\n",
    "for col in train.select_dtypes('object').columns:\n",
    "    value_counts = train[col].value_counts(normalize=True)\n",
    "\n",
    "    max = value_counts.max()\n",
    "    \n",
    "    print(value_counts)\n",
    "   # print(max)\n",
    "\n",
    "    if max > .70:\n",
    "        unbalanced_cols.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(unbalanced_cols,axis='columns',inplace=True)\n",
    "check_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(axis=0,inplace=True)\n",
    "print(train.shape)\n",
    "check_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.pop('SalePrice')\n",
    "x_train = train\n",
    "check_missing(train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 3us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 48s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 8s 2us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name='layers_flatten'),\n",
    "    tf.keras.layers.Dense(512, activation='relu', name='layers_dense'),\n",
    "    tf.keras.layers.Dropout(0.2, name='layers_dropout'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 35s 17ms/step - loss: 0.4975 - accuracy: 0.8226 - val_loss: 0.4266 - val_accuracy: 0.8441\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.3805 - accuracy: 0.8612 - val_loss: 0.3757 - val_accuracy: 0.8606\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.3487 - accuracy: 0.8719 - val_loss: 0.3597 - val_accuracy: 0.8681\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.3303 - accuracy: 0.8782 - val_loss: 0.3425 - val_accuracy: 0.8764\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.3145 - accuracy: 0.8834 - val_loss: 0.4075 - val_accuracy: 0.8540\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "\n",
    "  model = create_model()\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "  model.fit(x=x_train, \n",
    "            y=y_train, \n",
    "            epochs=5, \n",
    "            validation_data=(x_test, y_test), \n",
    "            callbacks=[tensorboard_callback])\n",
    "\n",
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "ccecdc02aa416300a64487fe6971a8398ebaf053d2b2768f769ac21dbbf13f86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
